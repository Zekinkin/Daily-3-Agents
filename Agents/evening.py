# ç¯å¢ƒé…ç½®
import os, feedparser, smtplib, datetime
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
from openai import OpenAI
from dotenv import load_dotenv
import time
import json
import random
from newspaper import Article
import requests
import re
from services.sheets import push_to_sheets
import datetime
from datetime import timedelta, timezone

# ================= ğŸ‡¨ğŸ‡³ åŒ—äº¬æ—¶é—´æ™ºèƒ½æ—¥æœŸé€»è¾‘ =================
beijing_tz = timezone(timedelta(hours=8))
now_in_beijing = datetime.datetime.now(beijing_tz)

if now_in_beijing.hour >= 18:
    target_date = now_in_beijing.date() + timedelta(days=1)
else:
    target_date = now_in_beijing.date()

today_str = target_date.strftime("%Y-%m-%d")
display_date_str = target_date.strftime('%A, %B %d, %Y')
# =========================================================

# å†å²è®°å½•æ–‡ä»¶ (é˜²æ­¢å‘é‡å¤çš„)
BASE_DIR = os.getcwd()
# âš ï¸ æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªæ–°æ–‡ä»¶åï¼ŒGitHub ä¼šè‡ªåŠ¨åˆ›å»ºå®ƒï¼Œä¸è¦æŒ‡å‘ä½ çš„ç´ ææºæ–‡ä»¶ï¼
HISTORY_FILE = os.path.join(BASE_DIR, "evening_history.json")

# å­—æ•°é™åˆ¶ (å•ä½ï¼šè‹±æ–‡å•è¯)
MIN_WORDS = 600
MAX_WORDS = 3000

client = OpenAI(
    api_key=os.getenv("DEEPSEEK_API_KEY"),
    base_url="https://api.deepseek.com"
)

# rssä¿¡æ¯æº
SAFE_RSS_SOURCES = [
    # --- ğŸŒŒ å®‡å®™ä¸æ·±ç©º (NASA & Webb) ---
    # NASA å®˜æ–¹æ–°é—»ï¼šæœ€æƒå¨çš„å®‡å®™æ¢ç´¢
    "https://www.nasa.gov/news-release/feed/",
    # éŸ¦ä¼¯æœ›è¿œé•œ (Webb)ï¼šæ¢ç´¢å®‡å®™èµ·æºï¼Œå›¾ç‰‡å’Œæ–‡å­—éƒ½æç¾
    "https://webbtelescope.org/news/news-releases?format=rss",
    # é’±å¾·æ‹‰ X å°„çº¿å¤©æ–‡å°ï¼šæ¢ç´¢é»‘æ´å’Œè¶…æ–°æ˜Ÿ
    "https://chandra.si.edu/press/rss.xml",

    # --- ğŸŒ¿ åœ°çƒä¸è‡ªç„¶ (USGS & FWS) ---
    # ç¾å›½åœ°è´¨è°ƒæŸ¥å±€ (USGS)ï¼šå…³äºç«å±±ã€åœ°éœ‡ã€çŸ¿ç‰©ã€åœ°è´¨å¥‡è§‚
    "https://www.usgs.gov/news/feed",
    # ç¾å›½é±¼ç±»åŠé‡ç”ŸåŠ¨ç‰©ç®¡ç†å±€ (FWS)ï¼šä¿æŠ¤æ¿’å±åŠ¨ç‰©ã€æ¹¿åœ°æ•…äº‹
    "https://www.fws.gov/news/rss",
    
    # --- ğŸ§¬ åŸºç¡€ç§‘å­¦ (NSF) ---
    # ç¾å›½å›½å®¶ç§‘å­¦åŸºé‡‘ä¼š (NSF)ï¼šå‰æ²¿ç§‘å­¦å‘ç°ï¼ˆç”Ÿç‰©ã€ç‰©ç†ã€æåœ°æ¢ç´¢ï¼‰
    "https://www.nsf.gov/rss/rss_www_news.xml",
    
    # --- â˜ï¸ å¤§æ°”ä¸æµ·æ´‹ (NOAA) ---
    # ç¾å›½å›½å®¶æµ·æ´‹å’Œå¤§æ°”ç®¡ç†å±€ (NOAA)ï¼šæµ·æ´‹æ·±å¤„ã€æ°”å€™ã€æå…‰
    "https://www.noaa.gov/news-releases/feed"
]


# è¿ç¦è¯åº“
BANNED_KEYWORDS = [
    # Politics & Geopolitics
    "trump", "biden", "election", "democrat", "republican", "senate", "congress",
    "white house", "putin", "xi jinping", "zelensky", "netanyahu",
    "ukraine", "russia", "gaza", "israel", "palestine", "hamas", "war", "military",
    "strike", "missile", "weapon", "sanction", "treaty", "diplomacy",
    "government", "politics", "policy", "parliament", "protest", "riot",
    
    # Crime & Violence
    "murder", "kill", "suicide", "assassinate", "terrorist", "terrorism",
    "bomb", "attack", "shooting", "gun", "crime", "victim", "abuse",
    
    # NSFW / Drugs / Gambling
    "sex", "porn", "erotic", "nude", "rape", "assault",
    "drug", "cocaine", "heroin", "marijuana", "cannabis", "opioid",
    "casino", "gambling", "betting", "lottery"
]


# è·å–æ–‡ç«  
def load_history():
    if os.path.exists(HISTORY_FILE):
        with open(HISTORY_FILE, 'r') as f:
            return set(json.load(f))
    return set()

def save_history(url):
    history = load_history()
    history.add(url)
    with open(HISTORY_FILE, 'w') as f:
        json.dump(list(history), f)

def is_content_safe(title, text):
    """
    ğŸ›¡ï¸ å®‰å…¨è¿‡æ»¤å™¨
    æ£€æŸ¥æ ‡é¢˜å’Œæ­£æ–‡ä¸­æ˜¯å¦åŒ…å«è¿ç¦è¯ã€‚
    è¿”å›: True (å®‰å…¨), False (ä¸å®‰å…¨)
    """
    # å°†æ–‡æœ¬è½¬ä¸ºå°å†™ä»¥ä¾¿åŒ¹é…
    content_blob = (title + " " + text).lower()
    
    for keyword in BANNED_KEYWORDS:
        # ä½¿ç”¨æ­£åˆ™è¿›è¡Œå•è¯è¾¹ç•ŒåŒ¹é…ï¼Œé˜²æ­¢è¯¯ä¼¤ (ä¾‹å¦‚ banned 'sex' ä¸åº”è¯¥åŒ¹é… 'essex')
        # \b åŒ¹é…å•è¯è¾¹ç•Œ
        if re.search(r'\b' + re.escape(keyword) + r'\b', content_blob):
            print(f"    âš ï¸ è§¦å‘æ•æ„Ÿè¯æ‹¦æˆª: [{keyword}]")
            return False
            
    return True

def get_filtered_article():
    print("ğŸŒ™ æ­£åœ¨å…¨ç½‘æœå¯»ä»Šæ™šçš„å®‡å®™ä¸è‡ªç„¶ (å«å®‰å…¨å®¡æŸ¥)...")
    
    sent_urls = load_history()
    shuffled_sources = random.sample(SAFE_RSS_SOURCES, len(SAFE_RSS_SOURCES))
    
    for url in shuffled_sources:
        try:
            print(f"  - æ­£åœ¨æ‰«ææº: {url} ...")
            feed = feedparser.parse(url)
            if not feed.entries: continue
            
            # æ¯ä¸ªæºåªçœ‹å‰ 3 ç¯‡ï¼Œé¿å…æµªè´¹æ—¶é—´
            for entry in feed.entries[:3]:
                link = entry.link
                title = entry.title
                
                # 1. å†å²æŸ¥é‡
                if link in sent_urls: continue
                
                # 2. æ ‡é¢˜åˆæ­¥å®¡æŸ¥ (çœæµé‡)
                if not is_content_safe(title, ""):
                    print(f"    âŒ æ ‡é¢˜åŒ…å«æ•æ„Ÿè¯ï¼Œè·³è¿‡: {title}")
                    continue

                try:
                    # æŠ“å–å…¨æ–‡
                    article = Article(link)
                    article.download()
                    article.parse()
                    text = article.text
                    word_count = len(text.split())
                    
                    # 3. å­—æ•°æ£€æŸ¥
                    if word_count < MIN_WORDS or word_count > MAX_WORDS:
                        # print(f"    âš ï¸ å­—æ•°ä¸ç¬¦ ({word_count}): {title}")
                        continue
                    
                    # 4. å…¨æ–‡æ·±åº¦å®¡æŸ¥ (Deep Check)
                    if not is_content_safe(title, text):
                        print(f"    âŒ æ­£æ–‡åŒ…å«æ•æ„Ÿè¯ï¼Œè·³è¿‡: {title}")
                        continue
                        
                    # âœ… å®Œç¾é€šè¿‡
                    print(f"    âœ… é€‰ä¸­æ–‡ç«  ({word_count}è¯): {title}")
                    return {
                        "title": article.title,
                        "author": entry.get("author", "Unknown"),
                        "source_name": feed.feed.get("title", "Science/Nature Source"),
                        "link": link,
                        "content": text
                    }
                    
                except Exception as e:
                    continue
                    
        except Exception:
            continue
            
    print("ğŸ˜­ æœªæ‰¾åˆ°åˆé€‚æ–‡ç« ã€‚")
    return None


# generate_evening_html
def generate_evening_html(article_data):
    print("ğŸ•¯ï¸ DeepSeek æ­£åœ¨ä¸ºä½ æ‹†è§£æ–‡ç« ï¼Œå‡†å¤‡ä¼´è¯» (æ³¨è¯»ç‰ˆ)...")
    
    # --- System Prompt ---
    system_prompt = """
    ä½ æ˜¯ä¸€ä½æ¸©æš–ã€åšå­¦çš„â€œæ™šé—´é˜…è¯»ä¼´ä¾£â€ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯å°†ä¸€ç¯‡è‹±æ–‡æ–‡ç« è½¬åŒ–ä¸ºâ€œæ³¨è¯»ç‰ˆâ€ç½‘é¡µï¼Œä¾›ç”¨æˆ·ç¡å‰é˜…è¯»ã€‚
    
    ã€æ’ç‰ˆæ ¸å¿ƒæŒ‡ä»¤ã€‘ï¼š
    1. **ç»å¯¹ç¦æ­¢ Markdown**ã€‚å¿…é¡»è¾“å‡ºçº¯ HTML ä»£ç ã€‚
    2. **è§†è§‰é£æ ¼**ï¼šè«å…°è¿ªæš–å’–è‰² (#fdfbf7)ï¼Œå­—ä½“å¼ºåˆ¶ä½¿ç”¨ Times New Romanã€‚
    3. **ç»“æ„é€»è¾‘**ï¼šä¸è¦å¼ºåˆ¶åˆ†ä¸º Part 1/2/3ã€‚è¯·æ ¹æ®æ–‡ç« çš„è‡ªç„¶æ®µè½é€»è¾‘ï¼Œå°†å…¶æ‹†åˆ†ä¸ºè‹¥å¹²ä¸ªâ€œé˜…è¯»å—â€ï¼ˆæ¯ä¸ªå—åŒ…å« 1-2 ä¸ªè‡ªç„¶æ®µï¼‰ã€‚
    """
    
    # --- User Prompt ---
    user_prompt = f"""
    ã€æ–‡ç« ä¿¡æ¯ã€‘ï¼š
    Title: {article_data['title']}
    Author: {article_data['author']}
    Source: {article_data['source_name']}
    Original Link: {article_data['link']}
    
    ã€æ–‡ç« å†…å®¹ã€‘ï¼š
    {article_data['content']}
    
    ã€å¤„ç†è¦æ±‚ã€‘ï¼š
    è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹æ–¹ HTML æ¨¡æ¿ç»“æ„è¾“å‡ºå®Œæ•´ä»£ç ã€‚
    
    1. **æ­£æ–‡å¤„ç† (Inline Annotations)**ï¼š
       - ä¿æŒè‹±æ–‡åŸæ–‡æµç•…ã€‚
       - é‡åˆ°é«˜é˜¶è¯æ±‡/éš¾è¯æ—¶ï¼Œ**ç›´æ¥åœ¨å•è¯å**æ·»åŠ ä¸­æ–‡é‡Šä¹‰ã€‚
       - æ ¼å¼è¦æ±‚ï¼šä½¿ç”¨ `<b>å•è¯</b><span style="color:#bc8a86; font-size: 0.9em;">ã€ä¸­æ–‡ã€‘</span>`ã€‚
       - ä¾‹å¦‚ï¼šThe sunset was <b>ephemeral</b><span style="color:#bc8a86; font-size: 0.9em;">ã€çŸ­æš‚çš„ã€‘</span>...
       
    2. **æŒ‰éœ€è¯­æ³•å¡ç‰‡ (Conditional Grammar Card)**ï¼š
       - åˆ†æå½“å‰æ®µè½æ˜¯å¦å­˜åœ¨**é•¿éš¾å¥**ï¼ˆç»“æ„å¤æ‚æˆ–å€’è£…/è™šæ‹Ÿè¯­æ°”ç­‰ï¼‰ã€‚
       - **å¦‚æœæœ‰**ï¼šåœ¨æ®µè½ä¸‹æ–¹æ’å…¥ä¸€ä¸ªâ€œè¯­æ³•è§£æå¡ç‰‡â€ï¼Œè§£é‡Šè¯¥å¥å­çš„ç»“æ„ã€‚
       - **å¦‚æœæ²¡æœ‰**ï¼šä¸è¦æ’å…¥å¡ç‰‡ï¼Œç›´æ¥ç»§ç»­ä¸‹ä¸€æ®µã€‚
       
    3. **ç»“å°¾**ï¼š
       - æå–ä¸€å¥æœ€æ²»æ„ˆçš„é‡‘å¥ (Golden Quote)ã€‚

    ã€HTML æ¨¡æ¿ä»£ç  (è¯·å¾ªç¯ç”Ÿæˆä¸­é—´çš„é˜…è¯»å—)ã€‘ï¼š
    
    <div style="background-color: #fdfbf7; padding: 40px 20px; font-family: 'Times New Roman', Times, serif; color: #2c2c2c; line-height: 2.0;">
        
        <div style="max-width: 650px; margin: 0 auto; text-align: center; margin-bottom: 50px; border-bottom: 1px solid #dcc1be; padding-bottom: 20px;">
            <div style="font-size: 12px; letter-spacing: 2px; color: #bc8a86; text-transform: uppercase; margin-bottom: 10px; font-family: sans-serif;">The Evening Read</div>
            <h1 style="font-size: 32px; color: #5d4037; margin-bottom: 15px; font-weight: normal; font-style: italic;">{article_data['title']}</h1>
            <p style="font-size: 14px; color: #999; font-family: sans-serif;">
                By {article_data['author']}
                <br><a href="{article_data['link']}" style="color: #bc8a86; text-decoration: none;">Read Original Source</a>
            </p>
        </div>

        <div style="max-width: 650px; margin: 0 auto;">
            
            <div style="margin-bottom: 35px;">
                
                <p style="font-size: 19px; text-align: justify; margin-bottom: 15px;">
                    (è¿™é‡Œå¡«å…¥åŸæ–‡æ®µè½... é‡åˆ°éš¾è¯è¯·ä½¿ç”¨ <b>word</b><span style="color:#bc8a86; font-size: 0.9em;">ã€ä¸­æ–‡ã€‘</span> æ ¼å¼æ ‡æ³¨...)
                </p>
                
                <div style="background-color: #f3ebe9; padding: 15px 20px; border-radius: 4px; font-family: sans-serif; font-size: 14px; color: #5d4037; border-left: 4px solid #bc8a86; margin-top: 10px;">
                    <div style="font-weight: bold; color: #bc8a86; margin-bottom: 5px;">ğŸ¦‰ Long Sentence Breakdown</div>
                    <div style="line-height: 1.6;">
                        (è¿™é‡Œå¼•ç”¨é‚£ä¸ªé•¿éš¾å¥)<br>
                        <span style="color: #888;">ğŸ‘‰ è§£æï¼š(ç®€è¦åˆ†æè¯­æ³•ç»“æ„ï¼Œå¦‚å®šè¯­ä»å¥ã€å€’è£…ç­‰)</span>
                    </div>
                </div>

            </div>
            <div style="text-align: center; margin-top: 60px; padding-top: 30px; border-top: 1px solid #dcc1be;">
                <p style="font-size: 20px; font-style: italic; color: #8d6e63; margin-bottom: 15px;">
                    " (è¯·æ‘˜å½•é‡‘å¥) "
                </p>
                <div style="font-size: 12px; color: #bc8a86; text-transform: uppercase; letter-spacing: 1px; font-family: sans-serif;">Goodnight & Sweet Dreams</div>
            </div>

        </div>
    </div>
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-reasoner",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            stream=False,
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
        return None
    

# run
def run():
    print("ğŸŒ™ æ™šæŠ¥ Agent å¯åŠ¨...")
    article_data = get_filtered_article() 
    
    if article_data:
        html_content = generate_evening_html(article_data)
        
        if html_content:
            
            # åªæœ‰ç”ŸæˆæˆåŠŸæ‰ä¿å­˜å†å²
            if article_data.get('link'):
                save_history(article_data['link'])
            
            # æ¨é€åˆ° Google Sheets
            subject = f"Evening Brief: {today_str}"
            push_to_sheets("evening", subject, html_content)
            print("ğŸ˜å·²pushåˆ°Google Sheet")
            
            return html_content
